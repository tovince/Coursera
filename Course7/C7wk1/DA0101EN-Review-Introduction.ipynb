{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<p style=\"text-align:center\">\n    <a href=\"https://skills.network\" target=\"_blank\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"Skills Network Logo\">\n    </a>\n</p>\n\n# Importing Dataset\n\nEstimated time needed: **15** minutes\n\n## Objectives\n\nAfter completing this lab you will be able to:\n\n-   Acquire data in various ways\n-   Obtain insights from data with Pandas library\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<h2>Table of Contents</h2>\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n<ol>\n    <li><a href=\"#Data-Acquisition\">Data Acquisition</a>\n    <li><a href=\"#Basic-Insights-from-the-Data-set\">Basic Insights from the Data set</a></li>\n</ol>\n\n\n</div>\n<hr>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Data Acquisition\n<p>\nA data set is typically a file containing data stored in one of several formats. Common file formats containing data sets include: .csv, .json, .xlsx etc. The data set can be stored in different places, on your local machine, on a server or a websiite, cloud storage and so on.<br>\n\nTo analyse data in a Python notebook, we need to bring the data set into the notebook. In this section, you will learn how to load a data set into our Jupyter Notebook.<br>\n\nIn our case, the Automobile Data set is an online source, and it is in a CSV (comma separated value) format. Let's use this data set as an example to practice reading data.\n<ul>\n    <li>Data source: <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\" target=\"_blank\">https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data</a></li>\n    <li>Data type: csv</li>\n</ul>\nThe Pandas Library is a very popular and very useful tool that enables us to read various datasets into a data frame; our Jupyter notebook platforms have a built-in <b>Pandas Library</b> so that all we need to do is import Pandas without installing.\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# uncomment the lines below if you need to install specific version of libraries if using this notebook \n# in an environment where these libraries are not installed \n#! mamba install pandas==1.3.3  -y\n#! mamba install numpy=1.21.2 -y",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# import pandas library\nimport pandas as pd\nimport numpy as np",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<h2>Read Data</h2>\n<p>\nWe utilize the <code>pandas.read_csv()</code> function for reading CSV files. However, in this version of the lab, which operates on JupyterLite, the dataset needs to be downloaded to the interface using the provided code below.\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This dataset was hosted on IBM Cloud object. Click <a href=\"https://cocl.us/DA101EN_object_storage\">HERE</a> for free storage.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The functions below will download the dataset into your browser:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from pyodide.http import pyfetch\n\nasync def download(url, filename):\n    response = await pyfetch(url)\n    if response.status == 200:\n        with open(filename, \"wb\") as f:\n            f.write(await response.bytes())",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "file_path='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/auto.csv'",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "To obtain the dataset, utilize the download() function as defined above:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "await download(file_path, \"auto.csv\")\nfile_name=\"auto.csv\"",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Utilize the Pandas method `read_csv()` to load the data into a dataframe.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv(file_name)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "> Note: This version of the lab is working on JupyterLite, which requires the dataset to be downloaded to the interface.While working on the downloaded version of this notebook on their local machines(Jupyter Anaconda), the learners can simply **skip the steps above,** and simply use the URL directly in the `pandas.read_csv()` function. You can uncomment and run the statements in the cell below.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#filepath = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/auto.csv\"\n#df = pd.read_csv(filepath, header=None)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "After reading the data set, we can use the <code>data_frame.head(n)</code> method to check the top n rows of the data frame, where n is an integer. Contrary to <code>data_frame.head(n)</code>, <code>data_frame.tail(n)</code> will show you the bottom n rows of the data frame.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# show the first 5 rows using dataframe.head() method\nprint(\"The first 5 rows of the dataframe\") \ndf.head(5)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h1> Question #1: </h1>\n<b>Check the bottom 10 rows of data frame \"df\".</b>\n</div>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Write your code below and press Shift+Enter to execute \n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for the solution</summary>\n\n```python\nprint(\"The last 10 rows of the dataframe\\n\")\ndf.tail(10)\n```\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<h3>Add Headers</h3>\n<p>\nTake a look at the data set. Pandas automatically set the header with an integer starting from 0.\n</p>\n<p>\nTo better describe the data, you can introduce a header. This information is available at:  <a href=\"https://archive.ics.uci.edu/ml/datasets/Automobile\" target=\"_blank\">https://archive.ics.uci.edu/ml/datasets/Automobile</a>.\n</p>\n<p>\nThus, you have to add headers manually.\n</p>\n<p>\nFirst, create a list \"headers\" that include all column names in order.\nThen, use <code>dataframe.columns = headers</code> to replace the headers with the list you created.\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# create headers list\nheaders = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]\nprint(\"headers\\n\", headers)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": " Replace headers and recheck our data frame:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.columns = headers\ndf.columns",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "You can also see the first 10 entries of the updated data frame and note that the headers are updated.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.head(10)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Now, we need to replace the \"?\" symbol with NaN so the dropna() can remove the missing values:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df1=df.replace('?',np.NaN)\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "You can drop missing values along the column \"price\" as follows:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df=df1.dropna(subset=[\"price\"], axis=0)\ndf.head(20)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Here, `axis=0` means that the contents along the entire row will be dropped wherever the entity 'price' is found to be NaN\n\nNow, you have successfully read the raw data set and added the correct headers into the data frame.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " <div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h1> Question #2: </h1>\n<b>Find the name of the columns of the dataframe.</b>\n</div>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Write your code below and press Shift+Enter to execute \n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for the solution</summary>\n\n```python\nprint(df.columns)\n```\n\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<h2>Save Dataset</h2>\n<p>\nCorrespondingly, Pandas enables you to save the data set to CSV. By using the <code>dataframe.to_csv()</code> method, you can add the file path and name along with quotation marks in the brackets.\n</p>\n<p>\nFor example, if you save the data frame <b>df</b> as <b>automobile.csv</b> to your local machine, you may use the syntax below, where <code>index = False</code> means the row names will not be written.\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "df.to_csv(\"automobile.csv\", index=False)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "You can also read and save other file formats. You can use similar functions like **`pd.read_csv()`** and **`df.to_csv()`** for other data formats. The functions are listed in the following table:\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<h2>Read/Save Other Data Formats</h2>\n\n| Data Formate |        Read       |            Save |\n| ------------ | :---------------: | --------------: |\n| csv          |  `pd.read_csv()`  |   `df.to_csv()` |\n| json         |  `pd.read_json()` |  `df.to_json()` |\n| excel        | `pd.read_excel()` | `df.to_excel()` |\n| hdf          |  `pd.read_hdf()`  |   `df.to_hdf()` |\n| sql          |  `pd.read_sql()`  |   `df.to_sql()` |\n| ...          |        ...        |             ... |\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Basic Insights from the Data set\n<p>\nAfter reading data into Pandas dataframe, it is time for you to explore the data set.<br>\n\nThere are several ways to obtain essential insights of the data to help you better understand it.\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<h2>Data Types</h2>\n<p>\nData has a variety of types.<br>\n\nThe main types stored in Pandas data frames are <b>object</b>, <b>float</b>, <b>int</b>, <b>bool</b> and <b>datetime64</b>. In order to better learn about each attribute, you should always know the data type of each column. In Pandas:\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.dtypes\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Returns a series with the data type of each column.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# check the data type of data frame \"df\" by .dtypes\nprint(df.dtypes)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<p>\nAs shown above, you can clearly to see that the data type of \"symboling\" and \"curb-weight\" are <code>int64</code>, \"normalized-losses\" is <code>object</code>, and \"wheel-base\" is <code>float64</code>, etc.\n</p>\n<p>\nThese data types can be changed; you will learn how to accomplish this in a later module.\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<h2>Describe</h2>\nIf we would like to get a statistical summary of each column such as count, column mean value, column standard deviation, etc., use the describe method:\n",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "dataframe.describe()",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This method will provide various summary statistics, excluding <code>NaN</code> (Not a Number) values.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.describe()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<p>\nThis shows the statistical summary of all numeric-typed (int, float) columns.<br>\n\nFor example, the attribute \"symboling\" has 205 counts, the mean value of this column is 0.83, the standard deviation is 1.25, the minimum value is -2, 25th percentile is 0, 50th percentile is 1, 75th percentile is 2, and the maximum value is 3.\n<br>\n\nHowever, what if you would also like to check all the columns including those that are of type object?\n<br><br>\n\nYou can add an argument <code>include = \"all\"</code> inside the bracket. Try it again.\n\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# describe all the columns in \"df\" \ndf.describe(include = \"all\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<p>\nNow it provides the statistical summary of all the columns, including object-typed attributes.<br>\n\nYOu can now see how many unique values there, which one is the top value, and the frequency of the top value in the object-typed columns.<br>\n\nSome values in the table above show \"NaN\". Those numbers are not available regarding a particular column type.<br>\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<div class=\"alert alert-danger alertdanger\" style=\"margin-top: 20px\">\n<h1> Question #3: </h1>\n\n<p>\nYou can select the columns of a dataframe by indicating the name of each column. For example, you can select the three columns as follows:\n</p>\n<p>\n    <code>dataframe[[' column 1 ',column 2', 'column 3']]</code>\n</p>\n<p>\nWhere \"column\" is the name of the column, you can apply the method  \".describe()\" to get the statistics of those columns as follows:\n</p>\n<p>\n    <code>dataframe[[' column 1 ',column 2', 'column 3'] ].describe()</code>\n</p>\n\nApply the  method to \".describe()\" to the columns 'length' and 'compression-ratio'.\n\n</div>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Write your code below and press Shift+Enter to execute \n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "\n<details><summary>Click here for the solution</summary>\n\n```python\ndf[['length', 'compression-ratio']].describe()\n```\n\n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<h2>Info</h2>\nYou can also use another method to check your data set:\n",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "dataframe.info()",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "It provides a concise summary of your data frame. \n\nThis method prints information about a data frame including the index dtype and columns, non-null values and memory usage.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# look at the info of \"df\"\ndf.info()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<h1>Excellent! You have just completed the  Introduction Notebook.</h1>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Thank you for completing this lab.\n\n## Author\n\n<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\" target=\"_blank\">Joseph Santarcangelo</a>\n\n### Other Contributors\n\n<a href=\"https://www.linkedin.com/in/mahdi-noorian-58219234/\" target=\"_blank\">Mahdi Noorian PhD</a>\n\nBahare Talayian\n\nEric Xiao\n\nSteven Dong\n\nParizad\n\nHima Vasudevan\n\n<a href=\"https://www.linkedin.com/in/fiorellawever/\" target=\"_blank\">Fiorella Wenver</a>\n\n<a href=\" https://www.linkedin.com/in/yi-leng-yao-84451275/ \" target=\"_blank\" >Yi Yao</a>.\n\n<a href=\"https://www.coursera.org/instructor/~129186572/ \" target=\"_blank\" >Abhishek Gagneja</a>\n## Change Log\n\n| Date (YYYY-MM-DD) | Version | Changed By | Change Description                       |\n| ----------------- | ------- | ---------- | ---------------------------------------- |\n| 2023-09-28        | 2.4     | Abhishek Gagneja| Minor instructional update          |\n| 2020-10-30        | 2.3     | Lakshmi    | Changed URL of the csv                   |\n| 2020-09-22        | 2.2     | Nayef      | Added replace() method to remove '?'     |\n| 2020-09-09        | 2.1     | Lakshmi    | Made changes in info method of dataframe |\n| 2020-08-27        | 2.0     | Lavanya    | Moved lab to course repo in GitLab       |\n\n\n<hr>\n\n## <h3 align=\"center\"> Â© IBM Corporation 2023. All rights reserved. <h3/>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}